{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recombine the final sequece and croos over between the weak links -> needed a custum function, maybe full deterministic\n",
    "- Stats and dataframe for the SNP and other variants\n",
    "- Implement gaps NNNNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio import SeqIO\n",
    "import yaml\n",
    "from math import modf\n",
    "from Bio.Seq import Seq\n",
    "from inspyred import swarm\n",
    "from inspyred import benchmarks\n",
    "from inspyred import ec\n",
    "from inspyred.ec import selectors\n",
    "from collections import deque\n",
    "from itertools import combinations\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from itertools import combinations\n",
    "from numba import njit\n",
    "import random\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed\n",
    "import seaborn as sns\n",
    "import torch as tr\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m read_tf_1\u001b[38;5;241m.\u001b[39mget_shape()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# read_tf = tf.strings.unicode_encode(read_tf, \"UTF-8\")\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mread_tf_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Implementation using tensors\n",
    "\n",
    "read_1 = \"ATTTAggggTA\"\n",
    "read_1 = read_1.upper()\n",
    "\n",
    "read_2 = \"ATTCGGATCGA\"\n",
    "read_2 = read_2.upper()\n",
    "\n",
    "read_tf_2 = tf.constant([ord(c) for c in read_2])\n",
    "read_tf_1 = tf.constant([ord(c) for c in read_1])\n",
    "read_tf_1.get_shape()[0]\n",
    "# read_tf = tf.strings.unicode_encode(read_tf, \"UTF-8\")\n",
    "\n",
    "for i in range(read_tf_1.get_shape()[0]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here trying parallelization with Numba e prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedRewriteError",
     "evalue": "\u001b[1mFailed in nopython mode pipeline (step: convert to parfors)\n\u001b[1mOverwrite of parallel loop index\n\u001b[1m\nFile \"..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_19680\\485280851.py\", line 331:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\dispatcher.py\", line 471, in _compile_for_args\n    error_rewrite(e, 'unsupported_error')\n  File \"c:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\dispatcher.py\", line 409, in error_rewrite\n    raise e.with_traceback(None)\nnumba.core.errors.UnsupportedRewriteError: Failed in nopython mode pipeline (step: convert to parfors)\n\u001b[1mOverwrite of parallel loop index\n\u001b[1m\nFile \"..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_19680\\485280851.py\", line 331:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mUnsupportedRewriteError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 348\u001b[0m\n\u001b[0;32m    344\u001b[0m comb \u001b[38;5;241m=\u001b[39m combinations(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(reads)),\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    346\u001b[0m real_comb \u001b[38;5;241m=\u001b[39m [(reads[i[\u001b[38;5;241m0\u001b[39m]], reads[i[\u001b[38;5;241m1\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m comb]\n\u001b[1;32m--> 348\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_align_func_par\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreal_comb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\filoa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mUnsupportedRewriteError\u001b[0m: \u001b[1mFailed in nopython mode pipeline (step: convert to parfors)\n\u001b[1mOverwrite of parallel loop index\n\u001b[1m\nFile \"..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_19680\\485280851.py\", line 331:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from numba import jit, prange, vectorize\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "import seaborn as sns\n",
    "import torch as tr\n",
    "import time\n",
    "\n",
    "seq = \"\"\n",
    "for seq_record in SeqIO.parse(\"C:\\\\Users\\\\filoa\\\\Desktop\\\\Programming_trials\\\\Assembler\\\\Data\\\\GCA_014117465.1_ASM1411746v1_genomic.fna\", format = \"fasta\"):\n",
    "    seq += str(seq_record.seq.upper())\n",
    "\n",
    "def comstum_reads(seq: str, length_reads = 10, coverage = 5, verbose = False) -> list:\n",
    "    \n",
    "    \"\"\"The function split the sequence in input in reads.\n",
    "    The splitting is done using random numbers, the amount of reds is given by: (len(seq)/length_read)*coverage.\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_reads = int(len(seq)/length_reads) * coverage\n",
    "    starting_pos = random.sample(range(0, len(seq)-length_reads+1), number_of_reads)\n",
    "    reads = []\n",
    "\n",
    "    for num in starting_pos:\n",
    "        reads.append(seq[num:num+length_reads])\n",
    "\n",
    "    if verbose == True:\n",
    "        # This part has the only aim to show some stats on the reads\n",
    "        y = [0 for i in range(0,len(seq)+1)]\n",
    "        for i in starting_pos:\n",
    "            for j in range(i, i+length_reads+1):\n",
    "                y[j] += 1 \n",
    "        sns.set_theme(style=\"darkgrid\")\n",
    "        sns.lineplot(y)\n",
    "        print(f\"There are {y.count(0)} bases that have 0 coverage.\")\n",
    "\n",
    "    return reads\n",
    "\n",
    "def tensor_score(ord_sequence:list, zeros:True)->int:\n",
    "    raise NotImplemented\n",
    "\n",
    "def tensor_align(sequence_one:str, sequence_two:str):\n",
    "    raise NotImplemented\n",
    "\n",
    "def uni_code(read:str)->np.ndarray:\n",
    "    return np.array([ord(c) for c in read])\n",
    "\n",
    "reads = comstum_reads(seq[:3000], length_reads = 150, coverage = 6)\n",
    "reads = [uni_code(read) for read in reads]\n",
    "\n",
    "### Vectorize e Numba\n",
    "\n",
    "@vectorize()\n",
    "def np_score(align_list: np.ndarray, zeros = True)->float:\n",
    "    \"\"\"This function is a replacement for the np function np.count_nonzero(), since inside the np_eval_function was needed to count the number of zeros (=matches).\n",
    "    However this function raise an error when run with the numba decorator.\n",
    "    \"\"\"\n",
    "    length = len(align_list)\n",
    "    cnt = 0\n",
    "\n",
    "    for i in align_list:\n",
    "        if i == 0:\n",
    "            cnt += 1\n",
    "\n",
    "    if zeros:\n",
    "        return float(cnt)\n",
    "    else:\n",
    "        return float(length-cnt)\n",
    "\n",
    "@jit(parallel=True)\n",
    "def np_align_func(seq_one:np.ndarray, seq_two:np.ndarray, match:int = 3, mismatch:int = -2) -> tuple:\n",
    "    \"\"\"This function is a replacement for the align function pirwise2.align.localms of the Bio library. This substitution has the aim of tackling the computational time of the\n",
    "    eval_alignment function. In order to decrease the time, there was the need to create a compilable function with numba, which was also capable of being parallelised.\n",
    "    As you can clearly see the function takes in input only the match and mismatch, because in this usage the gap introduction is useless.\n",
    "\n",
    "    seq_one, seq_two = input sequences already trasformed in byte\n",
    "    match, mismatch = integer value for the alignment\n",
    "\n",
    "    Note: the mismatch should be negative\n",
    "    Ex output: (12.0, 34, True)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialization of outputs, the output is a tuple that contains: score of the alignment, a number indicating how the two reads align\n",
    "    # and if the two sequences have been inverted in order during the process\n",
    "    score = float(0)\n",
    "    diff = 0\n",
    "    switch = False\n",
    "\n",
    "    # Since knowing which one is the longest is needed \n",
    "    if seq_one.shape[0] >= seq_two.shape[0]:\n",
    "        max_lenght_seq = seq_one.shape[0]\n",
    "        min_length_seq = seq_two.shape[0]\n",
    "\n",
    "    else:\n",
    "        switch = True\n",
    "        max_lenght_seq = seq_two.shape[0]\n",
    "        min_length_seq = seq_one.shape[0]\n",
    "        seq_one, seq_two = seq_two, seq_one\n",
    "    \n",
    "    # Number of iterations\n",
    "    num_iteration_int = (max_lenght_seq + min_length_seq - 1) // 2\n",
    "    num_iteration = (max_lenght_seq + min_length_seq - 1) / 2\n",
    "    alone = False\n",
    "\n",
    "    if num_iteration > num_iteration_int:\n",
    "        alone = True\n",
    "\n",
    "    for i in prange(num_iteration_int):\n",
    "        if i < min_length_seq:\n",
    "\n",
    "            align_forw = seq_one[:(i+1)] - seq_two[-(i+1):]\n",
    "            align_back = seq_two[:(i+1)] - seq_one[-(i+1):]\n",
    "\n",
    "            cnt = 0\n",
    "            for j in align_forw, align_back:\n",
    "                part_score = np_score(j)*match + np_score(j, zeros=False)*mismatch\n",
    "\n",
    "                if part_score >= score:\n",
    "                    score = part_score\n",
    "                    if cnt > 0:\n",
    "                        diff = max_lenght_seq -i -1\n",
    "                    else:\n",
    "                        diff = -(min_length_seq -i -1)\n",
    "                cnt += 1\n",
    "        \n",
    "        if i >= min_length_seq:\n",
    "            align_forw = seq_one[i-min_length_seq+1:(i+1)] - seq_two[-(i+1):]\n",
    "            align_back = seq_one[-(i+1):-(i-min_length_seq+1)] - seq_two[:(i+1)]\n",
    "\n",
    "            cnt = 0\n",
    "            for j in align_forw, align_back:\n",
    "                part_score = np_score(j)*match + np_score(j, zeros=False)*mismatch\n",
    "\n",
    "                \n",
    "                if part_score >= score:\n",
    "                    score = part_score\n",
    "                    if cnt > 0:\n",
    "                        diff = max_lenght_seq -i -1\n",
    "                    else:\n",
    "                        diff = -(min_length_seq -i -1)\n",
    "                cnt += 1 \n",
    "\n",
    "        if i == (num_iteration_int - 1) and alone:\n",
    "            i += 1\n",
    "\n",
    "            align_forw = seq_one[i-min_length_seq+1:(i+1)] - seq_two[-(i+1):]\n",
    "            part_score = np_score(j)*match + np_score(j, zeros=False)*mismatch\n",
    "\n",
    "            if part_score >= score:\n",
    "                score = part_score\n",
    "                diff = max_lenght_seq -i -1\n",
    "\n",
    "\n",
    "    return (score, diff, switch)\n",
    "\n",
    "\n",
    "def eval_allign_np(reads:list, par:list = [3, -2]) -> np.ndarray:\n",
    "    \"\"\"Funtion that evaulate the alignment\n",
    "\n",
    "    reads: list of DNA sequences, each of the read is a list of integers that resemble the real sequence\n",
    "\n",
    "    par: list of parameters to performe the alignment\n",
    "    es (the examples represent the defoult parameters):\n",
    "    match_score = 3,\n",
    "    mismatch_penalty = -2,\n",
    "\n",
    "    output:\n",
    "    Matrix with the weigts (distances) between the reads (nodes)\n",
    "    In this matrix there are both the scores of the alignment, recognizable for the tipical integer score (even if is a float point) and\n",
    "    a flaot number (like 0.23) which is needed after to recompose the sequence; it indicates the overlapping bases.\n",
    "    Ex:\n",
    "        allignment score -> 2.0, 13.0, ...\n",
    "        overlapping number -> 0.241, 0.61, 0.561, ...\n",
    "            To avoid problem later with 0 a 1 digit is added for then remove it. So 0.30 become 0.301 but the corret indices are 12 and 30.\n",
    "\n",
    "        These two numbers are link by the position in the matrix which are the trasposition\n",
    "        Score 14.0 in position (1,5) --> 0.34 in position (5,1). Only the score position is referred\n",
    "        to the direction of the edge.\n",
    "        1 ---> 5 with allignment score 14 and read_1 is overlapped with read_5 in positions 34 (both included)\n",
    "\n",
    "    Example of a matrix with three reads:\n",
    "\n",
    "        | 1    | 2    | 3    \n",
    "     1  | 0    |3.0   | 0.231 \n",
    "     2  | 0.601|  0   | 23.0\n",
    "     3  | 18.0 | 0.701|  0\n",
    "    \"\"\"\n",
    "    length = len(reads)\n",
    "    # initialization of the matrices\n",
    "    weigth_matrix = np.zeros((length, length))\n",
    "\n",
    "    # The score of the allingment of read[1] to read[2] is the same of the opposite (read[2] to read[1])\n",
    "    # So when the function found the diretionality of the allignment put the score in rigth spot and a 0 in the wrong one.\n",
    "    visited = collections.deque([j for j in range(length)])\n",
    "    # comb = combinations(range(len(reads)),2)\n",
    "    # for i,j in comb:\n",
    "\n",
    "    for i in tqdm(range(length)):\n",
    "\n",
    "        for j in visited:\n",
    "\n",
    "            if i == j:\n",
    "                # the diagonal of the matrix has 0 score because we are allinging the same sequence\n",
    "                continue\n",
    "            else:\n",
    "                # pairwise must return a positive score, if there is no one it return None\n",
    "                alignment = np_align_func(reads[i], reads[j], match = par[0], mismatch = par[1])\n",
    "\n",
    "                if alignment[2]:\n",
    "                    if alignment[1] > 0:\n",
    "                        weigth_matrix[j, i] = alignment[0]\n",
    "                        weigth_matrix[i, j] = float(f\"{0}.{abs(alignment[1])}1\")\n",
    "                    \n",
    "                    else:\n",
    "                        weigth_matrix[i, j] = alignment[0]\n",
    "                        weigth_matrix[j, i] = float(f\"{0}.{abs(alignment[1])}1\")\n",
    "\n",
    "                else:\n",
    "                    if alignment[1] > 0:\n",
    "                        weigth_matrix[i, j] = alignment[0]\n",
    "                        weigth_matrix[j, i] = float(f\"{0}.{abs(alignment[1])}1\")\n",
    "                    \n",
    "                    else:\n",
    "                        weigth_matrix[j, i] = alignment[0]\n",
    "                        weigth_matrix[i, j] = float(f\"{0}.{abs(alignment[1])}1\")\n",
    "\n",
    "                    \n",
    "        visited.popleft()\n",
    "    return weigth_matrix\n",
    "\n",
    "# eval_allign_np(reads = reads)\n",
    "\n",
    "### Parallel and delayed\n",
    "\n",
    "@jit(nopython = True)\n",
    "def np_score(align_list: np.ndarray, zeros = True)-> int:\n",
    "    \"\"\"This function is a replacement for the np function np.count_nonzero(), since inside the np_eval_function was needed to count the number of zeros (=matches).\n",
    "    However this function raise an error when run with the numba decorator.\n",
    "    \"\"\"\n",
    "    length = len(align_list)\n",
    "    cnt = 0\n",
    "\n",
    "    for i in align_list:\n",
    "        if i == 0:\n",
    "            cnt += 1\n",
    "\n",
    "    if zeros:\n",
    "        return cnt\n",
    "    else:\n",
    "        return length-cnt\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def np_align_func(seq_one:np.ndarray, seq_two:np.ndarray, match:int = 3, mismatch:int = -2) -> tuple:\n",
    "    \"\"\"This function is a replacement for the align function pirwise2.align.localms of the Bio library. This substitution has the aim of tackling the computational time of the\n",
    "    eval_alignment function. In order to decrease the time, there was the need to create a compilable function with numba, which was also capable of being parallelised.\n",
    "    As you can clearly see the function takes in input only the match and mismatch, because in this usage the gap introduction is useless (for the moment).\n",
    "    This function return only the BEST alignment.\n",
    "\n",
    "    seq_one, seq_two = input sequences already trasformed in integers by ord function\n",
    "    match, mismatch = integer value for the alignment\n",
    "\n",
    "    Note: the mismatch should be negative\n",
    "    Output: A tuple with the alignment score, a number that resamble the shift of the alignemnt, and a boolean which indicates if the order\n",
    "        of the input has been inverted or not. This last element is essential to retrive the order, so which of the two will be place before the other one.\n",
    "    Ex output: (12.0, 34, True)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialization of outputs, the output is a tuple that contains: score of the alignment, a number indicating how the two reads align\n",
    "    # and if the two sequences have been inverted in order during the process\n",
    "    score = 0\n",
    "    diff = 0\n",
    "    switch = False\n",
    "\n",
    "    # Since knowing which one is the longest is needed for the epoch\n",
    "    if seq_one.shape[0] >= seq_two.shape[0]:\n",
    "        max_lenght_seq = seq_one.shape[0]\n",
    "        min_length_seq = seq_two.shape[0]\n",
    "\n",
    "    else:\n",
    "        switch = True\n",
    "        max_lenght_seq = seq_two.shape[0]\n",
    "        min_length_seq = seq_one.shape[0]\n",
    "        seq_one, seq_two = seq_two, seq_one\n",
    "    \n",
    "    # Number of iterations (N + n -1)/2 because each iteration is producing two alignment one confronting the sequenvces from the forward\n",
    "    # and from the backward\n",
    "    num_iteration_int = (max_lenght_seq + min_length_seq - 1) // 2\n",
    "    num_iteration = (max_lenght_seq + min_length_seq - 1) / 2\n",
    "    alone = False # There could be needed an extra iteration if this (N + n -1)/2 is odd \n",
    "\n",
    "    if num_iteration > num_iteration_int:\n",
    "        alone = True\n",
    "\n",
    "    for i in range(num_iteration_int):\n",
    "        if i < min_length_seq:\n",
    "\n",
    "            # Back/Forward alignments, only overlapping bases are being used\n",
    "            align_forw = seq_one[:(i+1)] - seq_two[-(i+1):]\n",
    "            align_back = seq_two[:(i+1)] - seq_one[-(i+1):]\n",
    "\n",
    "            cnt = 0\n",
    "            for j in align_forw, align_back:\n",
    "                part_score = np_score(j)*match + np_score(j, zeros=False)*mismatch\n",
    "\n",
    "                if part_score >= score:\n",
    "                    score = part_score\n",
    "                    if cnt > 0:\n",
    "                        # If the diff value is positive the first sequence is upstream\n",
    "                        # The index diff is included\n",
    "                        diff = max_lenght_seq -i -1\n",
    "                    else:\n",
    "                        # If the diff value is negative the second sequence if the one upstream\n",
    "                        # The index diff is included\n",
    "                        diff = -(min_length_seq -i -1)\n",
    "                cnt += 1\n",
    "        \n",
    "        if i >= min_length_seq:\n",
    "            align_forw = seq_one[i-min_length_seq+1:(i+1)] - seq_two[-(i+1):]\n",
    "            align_back = seq_one[-(i+1):-(i-min_length_seq+1)] - seq_two[:(i+1)]\n",
    "\n",
    "            cnt = 0\n",
    "            for j in align_forw, align_back:\n",
    "                part_score = np_score(j)*match + np_score(j, zeros=False)*mismatch\n",
    "\n",
    "                \n",
    "                if part_score >= score:\n",
    "                    score = part_score\n",
    "                    if cnt > 0:\n",
    "                        diff = max_lenght_seq -i -1\n",
    "                    else:\n",
    "                        diff = -(min_length_seq -i -1)\n",
    "                cnt += 1 \n",
    "\n",
    "        if i == (num_iteration_int - 1) and alone:\n",
    "            i += 1\n",
    "\n",
    "            align_forw = seq_one[i-min_length_seq+1:(i+1)] - seq_two[-(i+1):]\n",
    "            part_score = np_score(j)*match + np_score(j, zeros=False)*mismatch\n",
    "\n",
    "            if part_score >= score:\n",
    "                score = part_score\n",
    "                diff = max_lenght_seq -i -1\n",
    "\n",
    "\n",
    "    return (score, diff, switch)\n",
    "\n",
    "\n",
    "def eval_allign_np(reads:list, par:list = [3, -2]) -> np.ndarray:\n",
    "    \"\"\"Funtion that evaulate the alignment\n",
    "\n",
    "    reads: list of DNA sequences, each of the read is a list of integers that resemble the real sequence\n",
    "\n",
    "    par: list of parameters to performe the alignment\n",
    "    es (the examples represent the defoult parameters):\n",
    "    match_score = 3,\n",
    "    mismatch_penalty = -2,\n",
    "\n",
    "    output:\n",
    "    Matrix with the weigts (distances) between the reads (nodes)\n",
    "    In this matrix there are both the scores of the alignment, recognizable for the tipical integer score (even if is a float point) and\n",
    "    a flaot number (like 0.23) which is needed after to recompose the sequence; it indicates the overlapping bases.\n",
    "    Ex:\n",
    "        allignment score -> 2.0, 13.0, ...\n",
    "        overlapping number -> 24.1, 6.1, 56.1, ...\n",
    "            To avoid problem later with 0 a 1 digit is added for then remove it. So 0.30 become 0.301 but the corret indices are 12 and 30.\n",
    "\n",
    "        These two numbers are link by the position in the matrix which are the trasposition\n",
    "        Score 14.0 in position (1,5) --> 34.1 in position (5,1). Only the score position is referred\n",
    "        to the direction of the edge.\n",
    "        1 ---> 5 with allignment score 14 and read_1 is overlapped with read_5 in positions 34 (both included)\n",
    "\n",
    "    Example of a matrix with three reads:\n",
    "\n",
    "        | 1    | 2    | 3    \n",
    "     1  | 0    |3.0   | 23.1 \n",
    "     2  | 60.1 |  0   | 23.0\n",
    "     3  | 18.0 | 70.1 |  0\n",
    "    \"\"\"\n",
    "    length = len(reads)\n",
    "    # initialization of the matrices\n",
    "    weigth_matrix = np.zeros((length, length))\n",
    "\n",
    "    # The score of the allingment of read[1] to read[2] is the same of the opposite (read[2] to read[1])\n",
    "    # So when the function found the diretionality of the allignment put the score in rigth spot and a 0 in the wrong one.\n",
    "    visited = collections.deque([j for j in range(length)])\n",
    "    # comb = combinations(range(len(reads)),2)\n",
    "    # for i,j in comb:\n",
    "\n",
    "    for i in tqdm(range(length)):\n",
    "\n",
    "        for j in visited:\n",
    "\n",
    "            if i == j:\n",
    "                # the diagonal of the matrix has 0 score because we are allinging the same sequence\n",
    "                continue\n",
    "            else:\n",
    "                # pairwise must return a positive score, if there is no one it return None\n",
    "                alignment = np_align_func(reads[i], reads[j], match = par[0], mismatch = par[1])\n",
    "\n",
    "                if alignment[0] > 0:\n",
    "\n",
    "                    if alignment[2]:\n",
    "                        # Swithch happend so reads[j] is longer then reads[i]\n",
    "                        if alignment[1] > 0:\n",
    "                            # cond = first sequence is upstream\n",
    "                            weigth_matrix[j, i] = alignment[0]\n",
    "                            weigth_matrix[i, j] = float(f\"{abs(alignment[1])}.1\")\n",
    "                        \n",
    "                        else:\n",
    "                            # cond = first sequence is downstream\n",
    "                            weigth_matrix[i, j] = alignment[0]\n",
    "                            weigth_matrix[j, i] = float(f\"{abs(alignment[1])}.1\")\n",
    "\n",
    "                    else:\n",
    "                        if alignment[1] > 0:\n",
    "                            # cond = first sequence is upstream\n",
    "                            weigth_matrix[i, j] = alignment[0]\n",
    "                            weigth_matrix[j, i] = float(f\"{abs(alignment[1])}.1\")\n",
    "                        \n",
    "                        else:\n",
    "                            # cond = first sequence is downstream\n",
    "                            weigth_matrix[j, i] = alignment[0]\n",
    "                            weigth_matrix[i, j] = float(f\"{abs(alignment[1])}.1\")\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                    \n",
    "        visited.popleft()\n",
    "    return weigth_matrix\n",
    "\n",
    "\n",
    "##########\n",
    "comb = combinations(range(len(reads)),2)\n",
    "\n",
    "real_comb = [(reads[i[0]], reads[i[1]]) for i in comb]\n",
    "\n",
    "results = Parallel(n_jobs=6)(delayed(np_align_func_par)(i) for i in real_comb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the final reconstruction function\n",
    "- return matrix\n",
    "- paralleize the coding/decoding in number\n",
    "- make a separe function for the consensus\n",
    "- probably is better to make lower the memory usage, since the consensus matrix will be in the same order of the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_consensus(path:list, reads:list, positions:list, length:int, max_coverage: int = 16, verbose:bool = False) ->str:\n",
    "    \"\"\"This function create a matrix and write down the numbers resembling the path found by the ants algorithm\n",
    "    \"\"\"\n",
    "    #Diff is included\n",
    "\n",
    "    cons_matrix = np.zeros((max_coverage, length))\n",
    "    leng = len(cons_matrix[0])\n",
    "    cum_dif = 0\n",
    "    adding = np.zeros((max_coverage, int(length/100)))\n",
    "\n",
    "    for i,j in path:\n",
    "        # Here i,j represent the edge of the graph, to retrive not the score but the alignment\n",
    "        # the function needs the opposite position where there are these informations matrix[j][i]\n",
    "\n",
    "        num = str(positions[j][i]).split(\".\")\n",
    "        dif = int(num[0])\n",
    "\n",
    "        if cons_matrix[0,0] == 0:\n",
    "            # This first part is for starting the writing of the matrix\n",
    "            \n",
    "            for pos in range(0, len(reads[i]) + 1):\n",
    "                if cons_matrix[0, pos] != 0:\n",
    "                    cons_matrix = np.append(cons_matrix, adding, 1)\n",
    "                cons_matrix[0, pos] = reads[i][pos]\n",
    "            cum_dif += dif\n",
    "            temp = 0\n",
    "            for p in range(cum_dif + 1, cum_dif + len(reads[j]) + 1): # added +1 in last mod\n",
    "                if cons_matrix[1,pos] != 0:\n",
    "                    cons_matrix = np.append(cons_matrix, adding, 1)\n",
    "                cons_matrix[1, p] = reads[j][temp]\n",
    "                temp += 1\n",
    "\n",
    "        else:\n",
    "            # There is a check if the initialized matrix is big enough to contain all tha bases, columns wise\n",
    "            if cons_matrix.shape[1] < cum_dif + len(reads[j])*2:\n",
    "                cons_matrix = np.append(cons_matrix, adding, 1)\n",
    "            else:\n",
    "                cum_dif += dif\n",
    "                temp = 0\n",
    "                for pos in range(cum_dif + 1, cum_dif + len(reads[j]) + 1): # added +1 in last mod\n",
    "                    row = 0\n",
    "                    while cons_matrix[row, pos] >= 1:\n",
    "                        row += 1\n",
    "                    # There is a check if the initialized matrix is big enough to contain all tha bases, row wise\n",
    "                    if row == cons_matrix.shape[0]:\n",
    "                        cons_matrix = np.append(cons_matrix, np.zeros((2, cons_matrix.shape[1])) ,0)\n",
    "                    cons_matrix[row, pos] = reads[j][temp]\n",
    "                    temp +=1\n",
    "\n",
    "    return cons_matrix\n",
    "\n",
    "\n",
    "def re_build(cons_matrix:list):\n",
    "    \n",
    "    dictionary = \"ATCG\"\n",
    "    cons_seq = \"\"\n",
    "    for i in range(0, len(cons_matrix)):\n",
    "        base = [x for x in cons_matrix[:,i] if x > 0]\n",
    "        if base == []:\n",
    "            return cons_seq\n",
    "        ind = []\n",
    "        for num in [ord(c) for c in dictionary]:\n",
    "            ind.append(base.count(num))\n",
    "        more_frequent = ind.index(max(ind))\n",
    "        # TODO stats\n",
    "        cons_seq += dictionary[more_frequent]\n",
    "\n",
    "    return cons_seq\n",
    "\n",
    "\n",
    "def join_consensus_sequence(consensus_matrix:np.ndarray, cpus:int=1)-> str:\n",
    "    \"This functio is just to implement the use of multiples core for recostructing the final sequence.\"\n",
    "\n",
    "    step = len(consensus_matrix)/cpus\n",
    "    cnt = 0\n",
    "    partials = []\n",
    "    for i in range(step, len(consensus_matrix) + step, step):\n",
    "        partials.append((cnt,i))\n",
    "        cnt += i\n",
    "    sub_parts = [consensus_matrix[:,i:j] for i,j in partials]\n",
    "    \n",
    "    results = Parallel(n_jobs=cpus)(delayed(re_build)(i) for i in sub_parts)\n",
    "\n",
    "    return \"\".join(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization of the string of bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTA\n",
      "[array([65, 84, 84, 65]), array([65, 84, 84, 65, 65]), array([65, 84, 84, 67, 71, 65, 84]), array([71, 71, 65, 71, 65, 71, 84, 67, 71, 71, 65])]\n",
      "['ATTA', 'ATTAA', 'ATTCGAT', 'GGAGAGTCGGA']\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "def uni_code(read:str)->np.ndarray:\n",
    "    return np.array([ord(c) for c in read])\n",
    "\n",
    "def de_code(read:np.ndarray)->str:\n",
    "    return \"\".join([chr(c) for c in read])\n",
    "\n",
    "reads = [\"ATTA\", \"ATTAA\", \"ATTCGAT\", \"GGAGAGTCGGA\"]\n",
    "\n",
    "print(de_code(uni_code(reads[0])))\n",
    "\n",
    "results = Parallel(n_jobs=5)(delayed(uni_code)(i)for i in reads)\n",
    "print(results)\n",
    "\n",
    "re_results = Parallel(n_jobs=5)(delayed(de_code)(j)for j in results)\n",
    "print(re_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembly class problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assembly_problem():\n",
    "    \"\"\"Defines the de novo genome assembly problem.\n",
    "    \n",
    "    This class based on the Traveling Salesman problem defines the problem\n",
    "    of assembling a new genome for which no reference is available (de novo assembly):\n",
    "    given a set of genomic reads and their pairwise overlap score, find the\n",
    "    path generating the longest consensus sequence. This problem assumes that \n",
    "    the ``weights`` parameter is an *n*-by-*n* matrix of pairwise \n",
    "    overlap among *n* reads. This problem is treated as a \n",
    "    maximization problem, socfitness values are determined to be the \n",
    "    proportional to the sum of the overlaps between each couple of reads\n",
    "    (the weight of the edge) and the length of the final assembled sequence.\n",
    "    \n",
    "    Public Attributes:c\n",
    "    \n",
    "    - *weights* -- the two-dimensional list of pairwise overlap \n",
    "    - *components* -- the set of ``TrailComponent`` objects constructed\n",
    "      from the ``weights`` attribute, where the element is the ((source,\n",
    "      destination), weight)\n",
    "    - *bias* -- the bias in selecting the component of maximum desirability\n",
    "      when constructing a candidate solution for ant colony optimization \n",
    "      (default 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reads, approximate_length):\n",
    "        self.weights = eval_allign(reads)\n",
    "        self.reads = reads\n",
    "        self.components = [swarm.TrailComponent((i, j), value=(self.weights[i][j])) for i, j in itertools.permutations(range(len(self.weights)), 2) if modf(self.weights[i,j])[0] == 0]\n",
    "        self.bias = 0.5\n",
    "        self.bounder = ec.DiscreteBounder([i for i in range(len(self.weights))])\n",
    "        self.best_path = None\n",
    "        self.maximize = True\n",
    "        self.length = approximate_length\n",
    "\n",
    "    def generator(self, random, args):\n",
    "            \"\"\"Return a candidate solution for an evolutionary computation.\"\"\"\n",
    "            locations = [i for i in range(len(self.weights))]\n",
    "            random.shuffle(locations)\n",
    "            return locations\n",
    "    \n",
    "    def constructor(self, random, args):\n",
    "        \"\"\"Return a candidate solution for an ant colony optimization.\"\"\"\n",
    "        self._use_ants = True\n",
    "        candidate = []\n",
    "        feasible_components = [1]   #Fake initialization to allow while loop to start\n",
    "        \n",
    "        # We need to visit all the nodes that CAN be visited, the graph is directed and not complete, meaning we can have no more nodes to visit without visiting all the\n",
    "        # nodes in the graph, thus, our termination condition is not visitin all the nodes but not having anymore feasible components\n",
    "        while len(feasible_components) > 0:\n",
    "            # At the start of the visit, all the components are feasible\n",
    "            if len(candidate) == 0:\n",
    "                feasible_components = self.components\n",
    "            elif len(candidate) == len(self.weights) - 1: # All the nodes have been visited\n",
    "                return candidate\n",
    "            else:\n",
    "                # Update feasible components and set of already visited nodes considering the node visited in the last iteration\n",
    "                last = candidate[-1]\n",
    "                already_visited = [c.element[0] for c in candidate]\n",
    "                already_visited.extend([c.element[1] for c in candidate])\n",
    "                already_visited = set(already_visited)\n",
    "                feasible_components = [c for c in self.components if c.element[0] == last.element[1] and c.element[1] not in already_visited]\n",
    "            if len(feasible_components) == 0:\n",
    "                return candidate\n",
    "            # Choose a feasible component\n",
    "            if random.random() <= self.bias:\n",
    "                next_component = max(feasible_components)\n",
    "            else:\n",
    "                next_component = selectors.fitness_proportionate_selection(random, feasible_components, {'num_selected': 1})[0]\n",
    "            candidate.append(next_component)\n",
    "        return candidate\n",
    "    \n",
    "    def cross_over(path:list, matrix:list)->list:\n",
    "        \"\"\"This function recombine the solution. Takes the path and the score associated to each edge\n",
    "        iterate over the path and switch two edge.\n",
    "        \"\"\"\n",
    "        imaginary_string = range(len(path))\n",
    "\n",
    "        min_1 = path.index(min([c.value for c in path]))\n",
    "        min_2 = path.index(min([c.value for c in path if (c.element[0] == min_1[0]) and (c.element[1] == min_1[1])]))\n",
    "        return None\n",
    "    \n",
    "    def evaluator(self, candidates, args):\n",
    "        \"\"\"Return the fitness values for the given candidates.\"\"\"\n",
    "        # TODO use normal distribution\n",
    "        fitness = []\n",
    "        for candidate in candidates:\n",
    "            total = 0\n",
    "            current_path = []\n",
    "            for c in candidate:\n",
    "                total += self.weights[c.element[0]][c.element[1]]\n",
    "                current_path.append(c)\n",
    "            # last = (candidate[-1].element[1], candidate[0].element[0])\n",
    "            # current_path=[(i.element[0],i.element[1]) for i in candidate] # maybe i is enough\n",
    "            # total += self.weights[last[0]][last[1]]\n",
    "            current_sequence = consensus_sequence(current_path, reads=self.reads, positions=self.weights, length=self.length)\n",
    "            length_score = abs((self.length-current_sequence)/self.length)\n",
    "            s = [5, 3, 1, 0.5, 0.2]\n",
    "            perc=[0, 0.01, 0.05, 0.08, 0.1, 0.2]\n",
    "            l_score = 0.1\n",
    "            for i in range(len(perc)-1):\n",
    "                if length_score >= perc[i] and length_score < perc[i+1]:\n",
    "                    l_score = s[perc.index(perc[i])]\n",
    "\n",
    "            if self.best_path == None:\n",
    "                self.best_path = current_path\n",
    "            \n",
    "            score = total * l_score\n",
    "            fitness.append(score)\n",
    "\n",
    "        return fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.0, 8.0, 7.0, 11.0, 10.0, 6.0, 15.0, 9.0, 23.0, 9.0, 6.0, 8.0, 3.0, 24.0, 8.0, 6.0, 21.0, 12.0, 9.0, 11.0, 11.0, 13.0, 10.0, 18.0, 15.0, 10.0, 15.0, 7.0, 11.0, 9.0, 4.0, 294.0, 12.0, 13.0, 7.0, 12.0, 12.0, 6.0, 11.0, 6.0, 12.0, 16.0, 13.0, 14.0, 10.0, 14.0, 9.0, 13.0, 11.0, 9.0, 17.0, 6.0, 17.0, 8.0, 9.0, 9.0, 14.0, 9.0, 3.0, 12.0, 10.0, 3.0, 7.0, 15.0, 9.0, 11.0, 8.0, 3.0, 114.0, 10.0, 11.0, 14.0, 4.0, 8.0, 9.0, 2.0, 4.0, 6.0, 13.0, 11.0, 6.0, 21.0, 7.0, 8.0, 7.0, 13.0, 6.0, 9.0, 17.0, 4.0, 13.0, 4.0, 13.0, 3.0, 19.0, 20.0, 5.0, 6.0, 16.0, 13.0, 3.0, 8.0, 4.0, 3.0, 11.0, 8.0, 11.0, 20.0, 5.0, 6.0, 4.0, 12.0, 5.0, 7.0, 9.0, 16.0, 13.0, 18.0, 3.0, 8.0, 6.0, 8.0, 14.0, 6.0, 9.0, 8.0, 6.0, 3.0, 14.0, 10.0, 15.0, 8.0, 4.0, 8.0, 12.0, 14.0, 25.0, 7.0, 7.0, 13.0, 8.0, 13.0, 9.0, 7.0, 339.0, 6.0, 10.0, 6.0, 11.0, 13.0, 6.0, 4.0, 8.0, 7.0, 16.0, 9.0, 12.0, 7.0, 11.0, 408.0, 15.0, 9.0, 10.0, 11.0, 7.0, 17.0, 4.0, 9.0, 6.0, 14.0, 6.0, 6.0, 4.0, 7.0, 3.0, 8.0, 10.0, 6.0, 3.0, 453.0, 10.0, 7.0, 22.0, 5.0, 11.0, 8.0, 3.0, 4.0, 9.0, 4.0, 4.0, 19.0, 20.0, 9.0, 9.0]\n",
      "195\n",
      "[14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 16.0, 16.0, 16.0, 16.0, 17.0, 17.0, 17.0, 17.0, 18.0, 18.0, 19.0, 19.0, 20.0, 20.0, 20.0, 21.0, 21.0, 22.0, 23.0, 24.0, 25.0, 114.0, 294.0, 339.0, 408.0, 453.0]\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# Simplification of the graph\n",
    "from scipy.io import loadmat\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from numba import jit, prange\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@jit(nopython = True)\n",
    "def __prepare_simpl_intup__(matrix:np.ndarray)->list:\n",
    "    \"\"\"Is needed to set up the parallelization, divide the matrix in columns\n",
    "    \"\"\"\n",
    "    # output = (array, len_array, column)\n",
    "\n",
    "    len_arrray = matrix.shape[0]\n",
    "    my_list = []\n",
    "\n",
    "    for i in range(len(matrix)):\n",
    "        my_list.append((list(matrix[:,i]), len_arrray, i))\n",
    "\n",
    "    return my_list\n",
    "\n",
    "\n",
    "def __matrix_selection__(input_tuple:tuple, cut_off = 0.2)->list:\n",
    "    \"\"\"Value the distibution of the columns, in this way select the ones\n",
    "    above the third quantile\n",
    "    \"\"\"\n",
    "\n",
    "    # Init\n",
    "    array = input_tuple[0]\n",
    "    array_len = input_tuple[1]\n",
    "    column = input_tuple[2]\n",
    "\n",
    "    # DO DO\n",
    "    links = [x for x in array if (x > 0) and (str(x).split(\".\")[1] == \"0\")]\n",
    "    chosen = sorted(links)[-(int(len(links)*cut_off)):]\n",
    "    print(f\"{links}\\n{len(links)}\\n{chosen}\\n{len(chosen)}\")\n",
    "    dissmissable_links = []\n",
    "\n",
    "    for i in links:\n",
    "        if i not in chosen:\n",
    "            dissmissable_links.append((array.index(i), column))\n",
    "\n",
    "    return dissmissable_links\n",
    "\n",
    "\n",
    "def __matrix_sempl__(matrix:np.ndarray, dissmissable_links:list) -> np.ndarray:\n",
    "    \"\"\"Changes the occurrencie valued as unprobable in zeros, eraising in this way the link\n",
    "    \"\"\"\n",
    "    for epoch in dissmissable_links:\n",
    "        for i,j in epoch:\n",
    "            matrix[i,j] = 0. \n",
    "            matrix[j,i] = 0.\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def graph_semplification(graph:np.ndarray, cores:int)->np.ndarray:\n",
    "    \n",
    "    list_of_tuple = __prepare_simpl_intup__(graph)\n",
    "\n",
    "    indeces_to_cut = Parallel(n_jobs = cores)(delayed(__matrix_selection__)(i) for i in list_of_tuple)\n",
    "\n",
    "    matrix = __matrix_sempl__(graph, dissmissable_links=indeces_to_cut)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "matrix = loadmat(\"C:\\\\Users\\\\filoa\\\\Desktop\\\\Programming_trials\\\\Assembler\\\\Data\\\\graph_metadata.mat\")[\"data\"]\n",
    "\n",
    "a = __prepare_simpl_intup__(matrix)\n",
    "\n",
    "b = __matrix_selection__(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "l = np.array([1,2,3,4,5,6])\n",
    "list(l)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
